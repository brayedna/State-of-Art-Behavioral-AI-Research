# ü§ñ State of the Art: Cr√©ation d'un Mod√®le Comportemental Bas√© sur l'Exp√©rience Utilisateur

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![AI Research](https://img.shields.io/badge/Research-AI%20Behavioral%20Modeling-blue)](https://github.com)
[![Team Project](https://img.shields.io/badge/Project-Team%20Research-green)](https://github.com)

## üìã R√©sum√© Ex√©cutif

Ce projet de recherche explore les m√©thodes avanc√©es d'intelligence artificielle pour cr√©er des mod√®les comportementaux qui imitent fid√®lement le comportement humain dans les environnements virtuels, particuli√®rement dans les jeux vid√©o. Notre √©tude comparative analyse 5 papiers de recherche majeurs pour identifier les approches les plus efficaces.

## üéØ Objectifs

- **Analyser** les diff√©rentes m√©thodes d'IA pour l'imitation du comportement humain
- **Comparer** les architectures d'apprentissage automatique (supervised, reinforcement, neuroevolution)
- **Identifier** les meilleures pratiques pour cr√©er des NPCs (Non-Player Characters) r√©alistes
- **√âvaluer** l'efficacit√© de diff√©rentes approches dans des contextes vari√©s

## üî¨ M√©thodologie

### Papiers de Recherche Analys√©s

| # | Titre | Domaine | M√©thode Principale | R√©sultats Cl√©s |
|---|-------|---------|-------------------|----------------|
| 1 | **AlphaStar** - Grandmaster level in StarCraft II | RTS | Multi-agent Reinforcement Learning | Premier IA √† atteindre le niveau Grandmaster |
| 2 | **TORCS** - Robust player imitation | Course automobile | Multi-objective Evolution (MOEA) | R√©sultats mitig√©s mais architecture int√©ressante |
| 3 | **Maia-Chess** - Aligning AI with human behavior | √âchecs | Supervised Learning + CNN | Pr√©diction pr√©cise du comportement humain |
| 4 | **Super Mario Bros** - Human behavior imitation | Plateforme | Neuroevolution + Dynamic Scripting | M√©thodes indirectes les plus efficaces |
| 5 | **Mario Controller** - Learning from human examples | Plateforme | Inverse Reinforcement Learning | Convergence rapide, adaptation aux erreurs |

### Approches Compar√©es

- **Apprentissage Supervis√©** : Entra√Ænement sur donn√©es humaines
- **Reinforcement Learning** : Optimisation par r√©compense
- **Neuroevolution** : √âvolution des r√©seaux de neurones
- **Dynamic Scripting** : Scripts adaptatifs
- **Multi-objective Optimization** : Optimisation multi-crit√®res

## üèóÔ∏è Architecture et Technologies

### Technologies Identifi√©es
- **Deep Learning** : CNN, RNN, architectures multi-agents
- **Reinforcement Learning** : Q-learning, policy gradient
- **Evolutionary Algorithms** : NSGA-II, MOEA
- **Computer Vision** : Traitement d'images de jeu
- **Neural Networks** : Random Forest, Multi-layer Perceptron

### Architectures Analys√©es
- **Multi-Agent Systems** (AlphaStar)
- **CNN Residual** (Maia-Chess)
- **Elman Networks** (TORCS)
- **Neuroevolutionary** (Super Mario)

## üìä R√©sultats et Insights

### D√©couvertes Cl√©s

1. **L'Apprentissage Supervis√©** sur donn√©es humaines est crucial pour l'imitation r√©aliste
2. **Les Limites Artificielles** (limitation layer) am√©liorent le r√©alisme comportemental
3. **Les M√©thodes Indirectes** (neuroevolution, dynamic scripting) surpassent l'imitation directe
4. **L'Inverse Reinforcement Learning** offre un bon compromis performance/adaptabilit√©
5. **L'Open Source** facilite la reproduction et l'am√©lioration des r√©sultats

### Recommandations

- Privil√©gier l'**optimisation de fonction de similarit√©** plut√¥t que l'imitation directe
- Int√©grer des **contraintes comportementales** pour √©viter la sur-performance
- Utiliser des **architectures multi-agents** pour les environnements complexes
- Combiner **plusieurs m√©thodes d'apprentissage** pour maximiser l'efficacit√©

## üë• √âquipe

**√âtudiants Ing√©nieurs - CESI √âcole d'ing√©nieur Strasbourg**

- **Mathieu** 
- **Brayan BIOUT**
- **Thibault**

**Tuteur :** Etienne

## üìö R√©f√©rences

1. Vinyals, O., et al. (2019). "Grandmaster level in StarCraft II using multi-agent reinforcement learning." *Nature*, 575, 350-354.
2. van Hoorn, N., et al. (2009). "Robust player imitation using multiobjective evolution." *IEEE Congress on Evolutionary Computation*.
3. McIlroy-Young, R., et al. (2020). "Aligning superhuman AI with human behavior: Chess as a model system." *KDD '20*.
4. Ortega, J., et al. (2013). "Imitating human playing styles in super mario bros." *Entertainment Computing*, 4, 93-104.
5. Lee, G., et al. (2014). "Learning a super mario controller from examples of human play." *IEEE Congress on Evolutionary Computation*.

## üéÆ Applications Pratiques

- **Jeux Vid√©o** : NPCs plus r√©alistes et immersifs
- **Simulation** : Mod√®les comportementaux pour la formation
- **Robotique** : Interfaces homme-machine plus naturelles
- **IA Explicable** : Compr√©hension des d√©cisions algorithmiques

## üîÆ Perspectives Futures

- Extension aux environnements 3D complexes
- Int√©gration de l'apprentissage continu
- D√©veloppement d'outils open-source
- Application √† d'autres domaines (sant√©, √©ducation)

## üìà Impact et Valorisation

Ce travail de recherche contribue √† l'√©tat de l'art en IA comportementale et offre des perspectives concr√®tes pour l'industrie du jeu vid√©o et au-del√†. Les m√©thodes identifi√©es peuvent √™tre appliqu√©es √† des projets industriels n√©cessitant des comportements humains r√©alistes.

---

**Mots-cl√©s :** `Machine Learning` `Human Behavior` `Video Games` `Reinforcement Learning` `Behavioral Modeling` `AI Research`

*Projet r√©alis√© dans le cadre du cursus ing√©nieur CESI Strasbourg - Mai 2022*
